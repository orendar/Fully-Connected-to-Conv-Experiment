{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oren/anaconda3/envs/py3/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from glob import glob\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def freeze(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def unfreeze(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "def load_image(filename) :\n",
    "    img = Image.open(filename)\n",
    "    img = img.convert('RGB')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model = models.vgg16_bn(pretrained=True)\n",
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('./datasets/images/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set()\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load the images and get the classnames from the image path\n",
    "for image in filenames:\n",
    "    class_name = image.rsplit(\"/\", 1)[1].rsplit('_', 1)[0]\n",
    "    classes.add(class_name)\n",
    "    img = load_image(image)\n",
    "\n",
    "    data.append(img)\n",
    "    labels.append(class_name)\n",
    "\n",
    "# convert classnames to indices\n",
    "class2idx = {cl: idx for idx, cl in enumerate(classes)}        \n",
    "labels = torch.Tensor(list(map(lambda x: class2idx[x], labels))).long()\n",
    "\n",
    "data = list(zip(data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    \"Dataset to serve individual images to our model\"\n",
    "    \n",
    "    def __init__(self, data, transforms=None):\n",
    "        self.data = data\n",
    "        self.len = len(data)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Since the data is not split into train and validation datasets we have to \n",
    "# make sure that when splitting between train and val that all classes are represented in both\n",
    "class Databasket:\n",
    "    \"Helper class to ensure equal distribution of classes in both train and validation datasets\"\n",
    "    \n",
    "    def __init__(self, data, num_cl, val_split=0.2, train_transforms=None, val_transforms=None):\n",
    "        class_values = [[] for x in range(num_cl)]\n",
    "        \n",
    "        # create arrays for each class type\n",
    "        for d in data:\n",
    "            class_values[d[1].item()].append(d)\n",
    "            \n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        \n",
    "        # put (1-val_split) of the images of each class into the train dataset\n",
    "        # and val_split of the images into the validation dataset\n",
    "        for class_dp in class_values:\n",
    "            split_idx = int(len(class_dp)*(1-val_split))\n",
    "            self.train_data += class_dp[:split_idx]\n",
    "            self.val_data += class_dp[split_idx:]\n",
    "            \n",
    "        self.train_ds = PetDataset(self.train_data, transforms=train_transforms)\n",
    "        self.val_ds = PetDataset(self.val_data, transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply transformations to the train dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# apply the same transformations to the validation set, with the exception of the\n",
    "# randomized transformation. We want the validation set to be consistent\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "databasket = Databasket(data, len(classes), val_split=0.2, train_transforms=train_transforms,\n",
    "                        val_transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def make_fcn_classifier(in_features, num_classes, convert_from_dense=False):\n",
    "    model = models.vgg16_bn(pretrained=True)\n",
    "    features = model.features\n",
    "    fcLayers = nn.Sequential(\n",
    "        # stop at last layer group\n",
    "        *list(model.classifier.children())[:-1]\n",
    "    )\n",
    "    fc = fcLayers[0].state_dict()\n",
    "    in_ch = in_features\n",
    "    out_ch = fc[\"weight\"].size(0)\n",
    "    assert out_ch == 4096\n",
    "    conv1 = nn.Conv2d(in_ch, 1024, kernel_size=3)\n",
    "    if convert_from_dense:\n",
    "        conv_fc6_weight = fc[\"weight\"].view(out_ch, in_ch, 7, 7)\n",
    "        conv_fc6_bias = fc[\"bias\"]\n",
    "        conv1_weights = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        conv1_bias = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        conv1.load_state_dict({\"weight\": conv1_weights,\n",
    "                               \"bias\": conv1_bias})\n",
    "        \n",
    "    return nn.Sequential(\n",
    "        conv1,\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=5),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    \n",
    "def requires_grad(layer):\n",
    "    \"Determines whether 'layer' requires gradients\"\n",
    "    ps = list(layer.parameters())\n",
    "    if not ps: return None\n",
    "    return ps[0].requires_grad\n",
    "\n",
    "def cnn_model(model, nc, convert_from_dense=False, init=nn.init.kaiming_normal_):\n",
    "    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "    \"Creates a model using a pretrained 'model' and appends a new head to it with 'nc' outputs\"\n",
    "    \n",
    "    # remove dense and freeze everything\n",
    "    body = nn.Sequential(*list(model.children())[:-2])\n",
    "    head = make_fcn_classifier(512, nc, convert_from_dense=convert_from_dense)\n",
    "    \n",
    "    model = nn.Sequential(body, head)\n",
    "    \n",
    "    # freeze the resnet34 base of the model\n",
    "    freeze_all(model[0].parameters())\n",
    "    \n",
    "    # initialize the weights of the head\n",
    "    for i, child in enumerate(model[1].children()):\n",
    "        if i == 0 and convert_from_dense:\n",
    "            freeze(child)\n",
    "            continue\n",
    "        if isinstance(child, nn.Module) and (not isinstance(child, bn_types)) and requires_grad(child): \n",
    "            init(child.weight)\n",
    "    \n",
    "    return model\n",
    "\n",
    "num_classes = len(classes)\n",
    "model = cnn_model(vgg_model, num_classes, convert_from_dense=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU(inplace=True)\n",
       "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (32): ReLU(inplace=True)\n",
       "      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (36): ReLU(inplace=True)\n",
       "      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (39): ReLU(inplace=True)\n",
       "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (42): ReLU(inplace=True)\n",
       "      (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(256, 37, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (10): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(range(len(databasket.train_ds)))\n",
    "test_indices = list(range(len(databasket.val_ds)))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "bs = 32\n",
    "\n",
    "# Basic dataloader to retrieve mini-batches from the datasets\n",
    "train_loader = DataLoader(databasket.train_ds, batch_size=bs,\n",
    "                          sampler=train_sampler, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(databasket.val_ds, batch_size=bs,\n",
    "                         sampler=test_sampler, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# We don't actually use the learning rate here. It's set to 1e-7 so that the LR Finder\n",
    "# starts at 1e-7\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c096a8f6d16c4ed49830b8fb4e875fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# used to find the ideal LR for our model training\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(train_loader, end_lr=3, num_iter=100)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A one cycle LR scheduler (https://arxiv.org/abs/1708.07120)\n",
    "# max_lr is derived from the lr_finder plot\n",
    "# epochs must match the amount of epochs you will run\n",
    "n_epochs = 15\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=8e-3, pct_start=0.3,\n",
    "                                          steps_per_epoch=len(train_loader), epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Accuracy: 5.3%, Validation Accuracy: 6.56%, Train Loss: 94.22, Validation Loss: 163.23\n",
      "Epoch 1: Train Accuracy: 6.48%, Validation Accuracy: 10.89%, Train Loss: 31.47, Validation Loss: 5.43\n",
      "Epoch 2: Train Accuracy: 16.22%, Validation Accuracy: 25.69%, Train Loss: 3.89, Validation Loss: 2.5\n",
      "Epoch 3: Train Accuracy: 31.2%, Validation Accuracy: 40.97%, Train Loss: 2.48, Validation Loss: 0.51\n",
      "Epoch 4: Train Accuracy: 44.41%, Validation Accuracy: 48.01%, Train Loss: 1.85, Validation Loss: 3.61\n",
      "Epoch 5: Train Accuracy: 55.85%, Validation Accuracy: 60.38%, Train Loss: 1.42, Validation Loss: 1.48\n",
      "Epoch 6: Train Accuracy: 62.83%, Validation Accuracy: 70.05%, Train Loss: 1.15, Validation Loss: 0.87\n",
      "Epoch 7: Train Accuracy: 68.82%, Validation Accuracy: 75.39%, Train Loss: 0.98, Validation Loss: 0.44\n",
      "Epoch 8: Train Accuracy: 74.2%, Validation Accuracy: 77.42%, Train Loss: 0.81, Validation Loss: 0.4\n",
      "Epoch 9: Train Accuracy: 75.96%, Validation Accuracy: 80.53%, Train Loss: 0.73, Validation Loss: 0.79\n",
      "Epoch 10: Train Accuracy: 79.56%, Validation Accuracy: 80.8%, Train Loss: 0.63, Validation Loss: 1.01\n",
      "Epoch 11: Train Accuracy: 81.54%, Validation Accuracy: 82.69%, Train Loss: 0.57, Validation Loss: 0.44\n",
      "Epoch 12: Train Accuracy: 83.83%, Validation Accuracy: 84.25%, Train Loss: 0.5, Validation Loss: 0.46\n",
      "Epoch 13: Train Accuracy: 84.17%, Validation Accuracy: 83.57%, Train Loss: 0.49, Validation Loss: 0.14\n",
      "Epoch 14: Train Accuracy: 84.18%, Validation Accuracy: 83.3%, Train Loss: 0.49, Validation Loss: 0.39\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "def train(epochs, scheduler, optimizer, model):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        n_correct = 0\n",
    "        \n",
    "        # use dropouts and batchnorms\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_acc = 100. * n_correct / len(databasket.train_ds)\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        n_val_correct = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # disable batchnorm and dropouts\n",
    "        model.eval()\n",
    "        # don't calculate gradient\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, labels).item()\n",
    "\n",
    "                n_val_correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "       \n",
    "        val_acc = 100. * n_val_correct / len(databasket.val_ds)   \n",
    "        print('Epoch {}: Train Accuracy: {}%, Validation Accuracy: {}%, Train Loss: {}, Validation Loss: {}' \n",
    "              .format(epoch, *np.around([train_acc, val_acc, train_loss, val_loss], 2)))\n",
    "        \n",
    "train(n_epochs, scheduler, optimizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_all(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Accuracy: 83.93%, Validation Accuracy: 84.58%, Train Loss: 0.48, Validation Loss: 1.53\n",
      "Epoch 1: Train Accuracy: 84.3%, Validation Accuracy: 84.31%, Train Loss: 0.48, Validation Loss: 0.3\n",
      "Epoch 2: Train Accuracy: 84.77%, Validation Accuracy: 84.72%, Train Loss: 0.47, Validation Loss: 0.27\n",
      "Epoch 3: Train Accuracy: 84.91%, Validation Accuracy: 85.6%, Train Loss: 0.46, Validation Loss: 1.19\n",
      "Epoch 4: Train Accuracy: 87.02%, Validation Accuracy: 86.27%, Train Loss: 0.38, Validation Loss: 0.22\n"
     ]
    }
   ],
   "source": [
    "# discriminative learning rate for fine tuning\n",
    "optimizer = optim.Adam([\n",
    "        {\"params\": model[0].parameters(), \"lr\": 1e-7},\n",
    "        {\"params\": model[1].parameters(), \"lr\": 1e-7}\n",
    "    ],  weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=(1e-6,1e-3), pct_start=0.8, steps_per_epoch=len(train_loader), epochs=5)\n",
    "train(5, scheduler, optimizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
