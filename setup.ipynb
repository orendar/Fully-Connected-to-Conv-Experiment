{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from glob import glob\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from livelossplot import PlotLosses\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all(model_params):\n",
    "    '''\n",
    "    Freeze entire network - taken from Skuldur.\n",
    "    '''\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_all(model_params):\n",
    "    '''\n",
    "    Unfreeze entire network - taken from Skuldur.\n",
    "    '''\n",
    "    for param in model_params:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def load_image(filename) :\n",
    "    '''\n",
    "    Load an image using PIL - taken from Skuldur.\n",
    "    '''\n",
    "    img = Image.open(filename)\n",
    "    img = img.convert('RGB')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data - taken from Skuldur.\n",
    "filenames = glob('./datasets/images/*.jpg')\n",
    "classes = set()\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load the images and get the classnames from the image path\n",
    "for image in filenames:\n",
    "    class_name = image.rsplit(\"/\", 1)[1].rsplit('_', 1)[0]\n",
    "    classes.add(class_name)\n",
    "    img = load_image(image)\n",
    "\n",
    "    data.append(img)\n",
    "    labels.append(class_name)\n",
    "\n",
    "# convert classnames to indices\n",
    "class2idx = {cl: idx for idx, cl in enumerate(classes)}        \n",
    "labels = torch.Tensor(list(map(lambda x: class2idx[x], labels))).long()\n",
    "\n",
    "data = list(zip(data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    '''\n",
    "    Dataset to serve individual images to our model - taken from Skuldur.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, transforms=None):\n",
    "        self.data = data\n",
    "        self.len = len(data)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Since the data is not split into train and validation datasets we have to \n",
    "# make sure that when splitting between train and val that all classes are represented in both\n",
    "class Databasket:\n",
    "    '''\n",
    "    Helper class to ensure equal distribution of classes\n",
    "    in both train and validation datasets - taken from Skuldur.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, num_cl, val_split=0.2, train_transforms=None, val_transforms=None):\n",
    "        class_values = [[] for x in range(num_cl)]\n",
    "        \n",
    "        # create arrays for each class type\n",
    "        for d in data:\n",
    "            class_values[d[1].item()].append(d)\n",
    "            \n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        \n",
    "        # put (1-val_split) of the images of each class into the train dataset\n",
    "        # and val_split of the images into the validation dataset\n",
    "        for class_dp in class_values:\n",
    "            split_idx = int(len(class_dp)*(1-val_split))\n",
    "            self.train_data += class_dp[:split_idx]\n",
    "            self.val_data += class_dp[split_idx:]\n",
    "            \n",
    "        self.train_ds = PetDataset(self.train_data, transforms=train_transforms)\n",
    "        self.val_ds = PetDataset(self.val_data, transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transforms - taken from Skuldur.\n",
    "# Apply transformations to the train dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# apply the same transformations to the validation set, with the exception of the\n",
    "# randomized transformation. We want the validation set to be consistent\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "databasket = Databasket(data, len(classes), val_split=0.2, train_transforms=train_transforms,\n",
    "                        val_transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "    Taken from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection.\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def make_fcn_classifier(in_features, num_classes, convert_from_dense=False):\n",
    "    '''\n",
    "    This function converts a VGG network into a fully-convolutional classifier\n",
    "    by replacing the linear head with a convolutional one.\n",
    "    Inspired by:\n",
    "    https://stackoverflow.com/questions/44146655/how-to-convert-pretrained-fc-layers-to-conv-layers-in-pytorch.\n",
    "    '''\n",
    "    model = models.vgg11_bn(pretrained=True)\n",
    "    features = model.features\n",
    "    fcLayers = nn.Sequential(\n",
    "        # stop at last layer group\n",
    "        *list(model.classifier.children())[:-1]\n",
    "    )\n",
    "    fc = fcLayers[0].state_dict()\n",
    "    in_ch = in_features\n",
    "    out_ch = fc[\"weight\"].size(0)\n",
    "    assert out_ch == 4096\n",
    "    p = 0.4\n",
    "    conv1 = nn.Conv2d(in_ch, 1024, kernel_size=3)\n",
    "    #if we are converting a dense layer into convolutional,\n",
    "    #then we reshape and decimate so that the shape fits.\n",
    "    #this is the same procedure as in the SSD paper.\n",
    "    if convert_from_dense:\n",
    "        conv_fc6_weight = fc[\"weight\"].view(out_ch, in_ch, 7, 7)\n",
    "        conv_fc6_bias = fc[\"bias\"]\n",
    "        conv1_weights = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        conv1_bias = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        conv1.load_state_dict({\"weight\": conv1_weights,\n",
    "                               \"bias\": conv1_bias})\n",
    "        \n",
    "    return nn.Sequential(\n",
    "        conv1,\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(p),\n",
    "        nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(p),\n",
    "        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(p),\n",
    "        nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=5),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    \n",
    "def requires_grad(layer):\n",
    "    '''\n",
    "    Determines whether 'layer' requires gradients - taken from Skuldur.\n",
    "    '''\n",
    "    ps = list(layer.parameters())\n",
    "    if not ps: return None\n",
    "    return ps[0].requires_grad\n",
    "\n",
    "def cnn_model(model, nc, convert_from_dense=False, init=nn.init.kaiming_normal_):\n",
    "    '''\n",
    "    This function has been modified from Skuldur to fit our purposes -\n",
    "    it takes the VGG backbone and appends to it the new convolutional head,\n",
    "    and then freezes all the pretrained layers \n",
    "    (potentially including the converted convolution) and initializes the rest.\n",
    "    '''\n",
    "    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "    \n",
    "    # remove dense and freeze everything\n",
    "    body = nn.Sequential(*list(model.children())[:-2])\n",
    "    head = make_fcn_classifier(512, nc, convert_from_dense=convert_from_dense)\n",
    "    \n",
    "    model = nn.Sequential(body, head)\n",
    "    \n",
    "    # freeze the base of the model\n",
    "    freeze_all(model[0].parameters())\n",
    "    \n",
    "    # initialize the weights of the head\n",
    "    for i, child in enumerate(model[1].children()):\n",
    "        if i == 0 and convert_from_dense:\n",
    "            freeze(child)\n",
    "            continue\n",
    "        if isinstance(child, nn.Module) and (not isinstance(child, bn_types)) and requires_grad(child): \n",
    "            init(child.weight)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#data setup - taken from Skuldur.\n",
    "train_indices = list(range(len(databasket.train_ds)))\n",
    "test_indices = list(range(len(databasket.val_ds)))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "bs = 32\n",
    "\n",
    "# Basic dataloader to retrieve mini-batches from the datasets\n",
    "trainloader = DataLoader(databasket.train_ds, batch_size=bs,\n",
    "                          sampler=train_sampler, shuffle=False, num_workers=0)\n",
    "testloader = DataLoader(databasket.val_ds, batch_size=bs,\n",
    "                         sampler=test_sampler, shuffle=False, num_workers=0)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": trainloader,\n",
    "    \"val\": testloader\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    '''\n",
    "    This function takes in a model, a loss function,\n",
    "    an optimizer, a learning-rate scheduler, and\n",
    "    a number of epochs, and runs the train loop\n",
    "    while plotting live training results.\n",
    "    Returns best model at the end of training.\n",
    "    Based on liveloss example.\n",
    "    '''\n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                #load data and labels\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "    \n",
    "                #run data through model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            prefix = ''\n",
    "            if phase == 'val':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
